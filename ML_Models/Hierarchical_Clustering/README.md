# ğŸ“˜ Hierarchical Clustering Algorithm

## ğŸ” About

Hierarchical Clustering is an **unsupervised machine learning algorithm** used to group data into a hierarchy of clusters. Unlike **K-Means**, it does not require specifying the number of clusters (*k*) in advance. Instead, it builds a tree-like structure called a **dendrogram**, which represents nested groupings of data points and their relationships.

The main idea is to **continuously merge or split clusters** based on their similarity (distance), forming a hierarchy from individual points (fine-grained) to one big cluster (broad view).

There are two major types:

* **Agglomerative (Bottom-Up):** Each data point starts as its own cluster, and the algorithm iteratively merges the closest clusters until all points are in one cluster.
* **Divisive (Top-Down):** Starts with all data points in one cluster and recursively splits them into smaller clusters.

---

## âš™ï¸ How It Works

1. **Start:**

   * Agglomerative â†’ each data point is its own cluster.
   * Divisive â†’ all data points are in one cluster.

2. **Measure Similarity:**

   * Use a **distance metric** (e.g., Euclidean, Manhattan, Cosine).
   * Define **linkage criteria** (how to measure distance between clusters):

     * *Single linkage* â†’ shortest distance between two points in clusters.
     * *Complete linkage* â†’ farthest distance between points.
     * *Average linkage* â†’ average distance between all pairs of points.
     * *Wardâ€™s method* â†’ minimizes variance between merged clusters.

3. **Merge or Split:**

   * Agglomerative â†’ merge the closest clusters step by step.
   * Divisive â†’ split clusters iteratively.

4. **Dendrogram Formation:**

   * Build a **tree-like structure** showing how clusters are combined at each step.
   * By cutting the dendrogram at a certain height, the desired number of clusters can be chosen.

---

## ğŸ§® Mathematical Intuition

Let two clusters be **Cáµ¢** and **Câ±¼**, with points *xâ‚š âˆˆ Cáµ¢* and *x\_q âˆˆ Câ±¼*.

* **Single Linkage Distance:**
  d(Cáµ¢, Câ±¼) = min ||xâ‚š âˆ’ x\_q||

* **Complete Linkage Distance:**
  d(Cáµ¢, Câ±¼) = max ||xâ‚š âˆ’ x\_q||

* **Average Linkage Distance:**
  d(Cáµ¢, Câ±¼) = (1 / (|Cáµ¢|Â·|Câ±¼|)) Î£ ||xâ‚š âˆ’ x\_q||

* **Wardâ€™s Method (Variance Minimization):**
  Î”E = (náµ¢Â·nâ±¼) / (náµ¢ + nâ±¼) Â· ||Î¼áµ¢ âˆ’ Î¼â±¼||Â²

Here:

* náµ¢, nâ±¼ â†’ number of points in clusters
* Î¼áµ¢, Î¼â±¼ â†’ centroids of clusters
* ||Î¼áµ¢ âˆ’ Î¼â±¼||Â² â†’ squared distance between centroids

This ensures clusters are merged in a way that **minimizes within-cluster variance**.

---

## âœ… Advantages

* No need to pre-specify the number of clusters (*unlike K-Means*).
* Provides a full hierarchical structure (dendrogram) to explore different levels of clustering.
* Works well with small datasets and gives interpretable results.
* Different linkage methods provide flexibility depending on data structure.

---

## âš ï¸ Limitations

* Computationally expensive (O(nÂ² log n)) â†’ not scalable for very large datasets.
* Sensitive to noise and outliers.
* Choice of distance metric and linkage method can significantly affect results.
* Once a merge or split is made, it cannot be undone (greedy approach).

---

## ğŸŒ Applications

* ğŸ§¬ **Bioinformatics:** Gene sequence clustering, phylogenetic tree construction.
* ğŸ“Š **Market Segmentation:** Grouping customers based on purchasing behavior.
* ğŸ“ **Text Mining & NLP:** Document or topic clustering.
* ğŸ¥ **Healthcare:** Grouping patients with similar medical conditions.
* ğŸ“ˆ **Finance:** Clustering stocks based on performance similarity.

---

âœ¨ **In summary:** Hierarchical Clustering is a powerful unsupervised technique for understanding the **structure and relationships in data**. It is especially useful when the number of clusters is not known beforehand and when interpretability (via dendrograms) is important.

---

# Hierarchical Clustering on Iris Dataset ğŸŒ¸

This notebook demonstrates **Hierarchical Clustering** using the classic **Iris dataset**. The workflow covers preprocessing, dimensionality reduction, dendrogram visualization, and evaluation of clustering performance.

ğŸ‘‰ Full notebook here: [Hierarchical Clustering Example](https://github.com/ashay-thamankar/ml_models/blob/main/ML_Models/Hierarchical_Clustering/Hierarchical_Clustering_Example.ipynb)

### ğŸ“Œ Key Steps Covered

* **Dataset**: Loaded the Iris dataset with 4 features.
* **Preprocessing**: Standardized the features for uniform scaling.
* **Dimensionality Reduction**: Applied **PCA** to reduce dimensions to 2 for visualization.
* **Dendrogram**: Constructed using **Wardâ€™s method** to visualize hierarchical relationships.
* **Agglomerative Clustering**: Implemented clustering with different cluster numbers.
* **Evaluation**: Computed **Silhouette Coefficients** to assess clustering quality across multiple cluster sizes.

### ğŸ¯ Objective

* Visualize how hierarchical clustering groups natural patterns in data.
* Understand the role of dendrograms in determining the optimal number of clusters.
* Compare clustering performance with different cluster choices.

### ğŸ“Š Insights

* Dendrogram provides a clear visual of how clusters merge.
* Silhouette scores help in choosing the most appropriate number of clusters.
* PCA ensures easy visualization while retaining key variance.

---
