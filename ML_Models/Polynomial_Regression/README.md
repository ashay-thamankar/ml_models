# ğŸ“˜ Polynomial Regression â€“ Mathematical Foundations & Interpretation

## ğŸ” What is Polynomial Regression?

Polynomial Regression is a type of **regression analysis** in which the relationship between the independent variable `x` and the dependent variable `y` is modeled as an **nth-degree polynomial**.

It is a form of **linear regression** where the **features are transformed**, but the model is still linear in terms of the parameters (coefficients).

---

## ğŸ“ Mathematical Formulation

### âœ… General Equation

The general form of a polynomial regression model of degree `n` is:

```
y = Î²â‚€ + Î²â‚x + Î²â‚‚xÂ² + Î²â‚ƒxÂ³ + ... + Î²â‚™xâ¿ + Îµ
```

Where:

* `y` = dependent variable
* `x` = independent variable
* `Î²â‚€, Î²â‚, ..., Î²â‚™` = model coefficients
* `Îµ` = error term (residual)

Despite the **non-linear curve** of the equation, it is considered a **linear model** because it is linear in its parameters (Î²â€™s).

---

## ğŸ”„ How it Works

1. **Feature Transformation**:

   * The original input variable `x` is **expanded** into higher-order terms: `xÂ²`, `xÂ³`, ..., `xâ¿`.

2. **Linear Regression on Transformed Features**:

   * The transformed variables are treated as separate input features, and a standard linear regression is applied.

---

## ğŸ§® Matrix Representation

Given:

* X: the design matrix (with polynomial terms)
* Î²: vector of coefficients
* y: target variable

The model can be written as:

```
y = XÎ² + Îµ
```

Where:

```
X = [1, x, xÂ², ..., xâ¿]
```

This formulation allows us to solve for `Î²` using the **Normal Equation**:

```
Î² = (Xáµ—X)â»Â¹Xáµ—y
```

---

## ğŸ“ˆ Why Use Polynomial Regression?

* **Captures non-linear relationships** between variables.
* More flexible than linear regression while retaining interpretability.
* Particularly useful when the data shows a **curved trend** that cannot be captured by a straight line.

---

## ğŸ“Œ Assumptions (same as Linear Regression)

Despite the polynomial terms, it still relies on linear regression assumptions:

1. **Linearity in parameters**: The model is linear in Î².
2. **Homoscedasticity**: Constant variance of residuals.
3. **Independence**: Observations are independent.
4. **Normality of errors**: Residuals are normally distributed.
5. **Low multicollinearity**: Especially relevant as powers of `x` can be highly correlated.

---

## ğŸ“Š Model Evaluation Metrics

* **RÂ² (Coefficient of Determination)** â€“ Measures the proportion of variance explained.
* **Adjusted RÂ²** â€“ Penalizes addition of unnecessary higher-order terms.
* **MSE / RMSE / MAE** â€“ Standard error metrics.
* **Residual Plots** â€“ Check for randomness of errors.
* **Cross-Validation** â€“ To prevent overfitting.

---

## âš ï¸ Common Pitfalls

* **Overfitting**:

  * Higher-degree polynomials may fit the training data too well and generalize poorly.

* **Multicollinearity**:

  * Powers of `x` can become strongly correlated (e.g., `x` vs. `xÂ²`), leading to instability in coefficient estimates.

* **Extrapolation Risk**:

  * Polynomial models are unreliable outside the range of training data due to extreme curvature.

---

## ğŸ§  Bias-Variance Tradeoff

* **Low-degree polynomial**: May underfit the data â†’ High bias, low variance.
* **High-degree polynomial**: May overfit the data â†’ Low bias, high variance.

Choosing the **right degree** involves **cross-validation** and understanding the dataâ€™s structure.

---

## âœ… When to Use Polynomial Regression

* You suspect or observe a **curved relationship** between `x` and `y`.
* Residual plots from linear regression indicate **non-random patterns**.
* You want a **simple, interpretable** model compared to more complex non-linear models like SVR, trees, or neural networks.

---

## ğŸ§© Alternatives to Polynomial Regression

* **Spline Regression** â€“ Piecewise polynomial fitting.
* **Generalized Additive Models (GAMs)** â€“ Combine flexibility of splines with additive structure.
* **Decision Trees / Random Forests** â€“ Non-parametric models that capture non-linearity.
* **Support Vector Regression (SVR)** â€“ Works well for non-linear regression.

---

## ğŸ“ Summary

* Polynomial Regression is a **powerful extension** of linear regression.
* It enables us to fit **non-linear patterns** by transforming input features.
* While easy to implement, it must be used **cautiously** to avoid overfitting and multicollinearity.
* Always validate using **cross-validation** and interpret results using residuals and performance metrics.

# ğŸ§ª Polynomial Regression â€“ Practical Implementation

## ğŸ“Š Step 1: Dataset Creation

* A dataset is synthetically generated with 100 samples, where `X` values are randomly drawn from a uniform distribution between `-3` and `+3`.

* The target variable `y` is generated using a **second-degree polynomial equation**:

  ```
  y = 0.5xÂ² + 1.5x + 2 + noise
  ```

* Random noise is added using a standard normal distribution to simulate real-world data imperfections.

---

## ğŸ”€ Step 2: Train-Test Split

* The data is split into training (80%) and testing (20%) sets.
* This allows evaluation of model performance on unseen data and avoids overfitting.

---

## ğŸ“ˆ Step 3: Linear Regression (Baseline)

* A **Simple Linear Regression** model is trained on the training data.
* Since the true relationship is quadratic, a straight-line model won't fully capture the curve.
* The **RÂ² score** on the test data shows the model explains only a portion of the variance.
* A visual plot confirms the poor fit: the straight line does not follow the curved trend in the data.

---

## ğŸ“‰ Step 4: Polynomial Regression (Degree = 2)

* The input features are transformed using **PolynomialFeatures** with degree 2.
* This adds `xÂ²` as a new feature, enabling the model to capture the quadratic nature.
* A new **Linear Regression** is trained on the transformed features.
* The **RÂ² score significantly improves**, indicating a much better fit to the test data.
* The model coefficients show that:

  * `Î²â‚€ â‰ˆ 1.96` (intercept)
  * `Î²â‚ â‰ˆ 1.51` (linear term)
  * `Î²â‚‚ â‰ˆ 0.48` (quadratic term)
* This aligns closely with the true data-generating equation.

---

## ğŸ” Step 5: Polynomial Regression (Degree = 3)

* To explore whether a **higher-degree** polynomial performs better, the degree is increased to 3.
* The third-degree term (`xÂ³`) is added as a feature.
* The **RÂ² score is slightly lower** than degree 2, suggesting possible **overfitting**.
* This illustrates the **bias-variance tradeoff**: higher complexity does not always improve generalization.

---

## ğŸ“Š Step 6: New Predictions & Visualization

* A new set of `X` values in the range `[-3, 3]` is generated.
* The trained polynomial model (degree 3) is used to make predictions.
* A line plot is drawn showing:

  * The polynomial regression prediction curve
  * The original training and test data points
* This visualization helps confirm how well the model captures the underlying trend.

---

## ğŸ§° Step 7: Using Pipelines for Simplification

* A **scikit-learn Pipeline** is introduced to streamline the workflow:

  * First step: `PolynomialFeatures` transformation
  * Second step: `LinearRegression` fitting
* The pipeline encapsulates both transformations and modeling into one object.
* A function is defined to generate and visualize models of varying polynomial degrees using this pipeline.
* The model with **degree 6** is plotted to observe how complexity increases curve fitting.

---

## ğŸ“Œ Key Takeaways

* Polynomial Regression can accurately model **non-linear relationships** using linear algorithms.
* Model complexity (degree of the polynomial) should be chosen carefully to balance **fit vs generalization**.
* Pipelines make the modeling process **modular and reusable**, especially when combining multiple steps.
* Visualizations and RÂ² scores provide insight into how well a model fits the data.

---

## ğŸ“ Useful Concepts Covered

* Feature transformation using `PolynomialFeatures`
* Model training with `LinearRegression`
* Model evaluation using `r2_score`
* Data visualization using `matplotlib`
* Workflow automation using `Pipeline`

