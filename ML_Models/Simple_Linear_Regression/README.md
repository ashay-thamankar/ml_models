# ğŸ“˜ Linear Regression â€“ Detailed Explanation

## ğŸ” Overview

**Linear Regression** is one of the most fundamental and widely used statistical and machine learning algorithms. It models the relationship between a **dependent variable (target)** and one or more **independent variables (features)** by fitting a straight line (in higher dimensions, a hyperplane) to the data.

---

## ğŸ“ˆ Objective

To find the **best-fitting line** that minimizes the difference between the actual and predicted values of the dependent variable.

---

## ğŸ“ Equation

### â¤ **Simple Linear Regression (One Independent Variable)**:

â€ƒâ€ƒ**y = Î²â‚€ + Î²â‚x + Îµ**

Where:

* **y** = Dependent variable (target)
* **x** = Independent variable (feature)
* **Î²â‚€** = Intercept (value of y when x = 0)
* **Î²â‚** = Slope (change in y per unit change in x)
* **Îµ** = Error term (difference between actual and predicted)

### â¤ **Multiple Linear Regression (Multiple Features)**:

â€ƒâ€ƒ**y = Î²â‚€ + Î²â‚xâ‚ + Î²â‚‚xâ‚‚ + ... + Î²â‚™xâ‚™ + Îµ**

---

## âš™ï¸ How It Works

Linear regression finds the best coefficients (**Î²**) by **minimizing the error** between predicted and actual values. This is done using:

### âœ”ï¸ **Ordinary Least Squares (OLS)**

OLS minimizes the **sum of squared residuals**:
â€ƒâ€ƒ**Loss Function**:
â€ƒâ€ƒ**L = Î£(yáµ¢ - Å·áµ¢)Â²**
Where:

* **yáµ¢** = actual value
* **Å·áµ¢** = predicted value
* **(yáµ¢ - Å·áµ¢)** = residual/error

The model uses calculus (partial derivatives and gradient descent or matrix algebra) to find the optimal **Î²** values that minimize this loss.

---

## ğŸ“Œ Assumptions of Linear Regression

1. **Linearity**: Relationship between dependent and independent variables is linear.
2. **Homoscedasticity**: Constant variance of errors across all levels of the independent variables.
3. **Independence**: Observations are independent of each other.
4. **No multicollinearity**: Independent variables are not highly correlated with each other.
5. **Normality of errors**: Errors (residuals) are normally distributed.

---

## ğŸ§ª Evaluation Metrics

To assess model performance:

| Metric       | Description                                   |
| ------------ | --------------------------------------------- |
| **RÂ² Score** | Proportion of variance explained by the model |
| **MAE**      | Mean Absolute Error                           |
| **MSE**      | Mean Squared Error                            |
| **RMSE**     | Root Mean Squared Error                       |

---

## âœ… Advantages

* Easy to understand and interpret
* Computationally efficient
* Requires fewer resources
* Serves as a good baseline model
* Works well with linearly separable data

---

## âš ï¸ Limitations

* Assumes linear relationships
* Sensitive to outliers
* Prone to underfitting for complex data
* Cannot handle multicollinearity well (in multiple regression)
* Poor performance if assumptions are violated

---

## ğŸ¯ Applications

* House price prediction
* Stock price trend estimation
* Salary prediction based on experience
* Demand forecasting
* Risk assessment in finance and insurance
* Medical cost estimation

---

## ğŸ“ Example Use Case in Notebook

In this notebook:

* We import data and preprocess it
* Use `scikit-learn` to build a Linear Regression model
* Fit the model and interpret the coefficients
* Visualize the line of best fit
* Evaluate using RÂ², MAE, and RMSE
* Discuss residual plots and assumption checks

---

## ğŸ§ª Notebook Summary: Linear Regression Example

This notebook demonstrates **Simple Linear Regression** using a dataset of **Height vs. Weight**.

### ğŸ“‹ Steps Covered:

1. **Data Loading & Visualization**

   * Loaded `height-weight.csv`
   * Created scatter plots to visualize the relationship
   * Found a strong positive correlation (0.93) between Weight and Height

2. **Data Preparation**

   * Defined Weight as the **independent variable (X)** and Height as the **dependent variable (y)**
   * Split the data into training and testing sets (75/25)
   * Standardized the input feature using `StandardScaler`

3. **Model Training**

   * Applied `LinearRegression()` from `sklearn`
   * Fitted the model on training data
   * Learned coefficients:

     * **Slope (Î²â‚)**: \~17.29
     * **Intercept (Î²â‚€)**: \~156.47

4. **Model Evaluation**

   * Predicted values for test data
   * Calculated:

     * **MSE**: 114.84
     * **MAE**: 9.67
     * **RMSE**: 10.71
     * **RÂ² Score**: 0.736
     * **Adjusted RÂ²**: 0.67

5. **OLS (Statsmodels) Comparison**

   * Also fitted the model using `statsmodels.OLS`
   * Generated detailed statistical summary (p-values, t-stats, RÂ², etc.)
   * Note: RÂ² was low due to model fitted **without intercept**

6. **New Prediction**

   * Predicted Height for new Weight value (72 kg):
     â¤ **Predicted Height â‰ˆ 155.98 cm**

---

### ğŸ” Key Insight:

Linear Regression successfully captured the positive relationship between weight and height, showing good predictive power with a relatively high RÂ² value.

---

## ğŸ“š Conclusion

Linear Regression is a powerful yet simple algorithm for modeling relationships in data. While itâ€™s easy to deploy and interpret, itâ€™s essential to validate assumptions and understand its limitations. It's often the first step in understanding the structure of data before moving to more complex models.

---

