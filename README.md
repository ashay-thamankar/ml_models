# Machine Learning Models ðŸ“˜

This repository contains implementations of various **Machine Learning algorithms**, categorized into **Supervised Learning** and **Unsupervised Learning**.

---

## ðŸ”¹ Supervised Learning
Supervised learning uses **labeled data** (input â†’ output pairs). The model learns from training data and predicts outcomes for new data.  
Examples include **classification** (predicting categories) and **regression** (predicting continuous values).

| Algorithm | Short description | Link |
|-----------|-------------------|------|
| Simple Linear Regression | Predicts a continuous value using one feature | [Simple_Linear_Regression](./ML_Models/Simple_Linear_Regression/) |
| Multiple Linear Regression | Predicts a continuous value using multiple features | [Multiple_Linear_Regression](./ML_Models/Multiple_Linear_Regression/) |
| Polynomial Regression | Extends linear regression with polynomial features | [Polynomial_Regression](./ML_Models/Polynomial_Regression/) |
| Ridge & Lasso Regression | Regularized linear regression to prevent overfitting | [Ridge_Lasso_Regression](./ML_Models/Ridge_Lasso_Regression/) |
| Elastic Net Regression | Combines Ridge (L2) and Lasso (L1) penalties | [Elastic_Net_Regression](./ML_Models/Elastic_Net_Regression/) |
| Logistic Regression | Linear model for binary/multi-class classification | [Logistic_Regression](./ML_Models/Logistic_Regression/) |
| Naive Bayes Classifier | Probabilistic classifier using Bayes' theorem | [Naive_Bayes_Classifier](./ML_Models/Naive_Bayes_Classifier/) |
| K-Nearest Neighbour (KNN) | Classifies based on the closest training examples | [K_Nearest_Neighbour](./ML_Models/K_Nearest_Neighbour/) |
| Support Vector Machine (SVM) | Finds the optimal boundary between classes | [SVM](./ML_Models/SVM/) |
| Decision Tree | Tree-based model for classification/regression | [Decision_Tree](./ML_Models/Decision_Tree/) |
| Random Forest | Ensemble of decision trees for stability | [Random_Forest](./ML_Models/Random_Forest/) |
| ADA Boost | Boosting method to improve weak learners | [ADA_Boost](./ML_Models/ADA_Boost/) |
| Gradient Boost | Boosting that corrects previous errors | [Gradient_Boost](./ML_Models/Gradient_Boost/) |
| XGBoost | Optimized gradient boosting algorithm | [XgBoost](./ML_Models/XgBoost/) |

---

## ðŸ”¹ Unsupervised Learning
Unsupervised learning works on **unlabeled data**. It finds hidden patterns, groupings, or structures without predefined outputs.  
Examples include **clustering** and **dimensionality reduction**.

| Algorithm | Short description | Link |
|-----------|-------------------|------|
| K-Means Clustering | Groups data into K clusters by similarity | [Kmeans_Clustering](./ML_Models/Kmeans_Clustering/) |
| Hierarchical Clustering | Builds nested clusters (tree-like structure) | [Hierarchical_Clustering](./ML_Models/Hierarchical_Clustering/) |
| DBSCAN Clustering | Finds clusters of arbitrary shape & handles noise | [DBSCAN_Clustering](./ML_Models/DBSCAN_Clustering/) |
| Principal Component Analysis (PCA) | Reduces dimensions while preserving variance | [Principal_Component_Analysis](./ML_Models/Principal_Component_Analysis/) |

---

## ðŸš€ How to Use
1. Navigate to the desired model folder.  
2. Each folder contains:  
   - `README.md` â†’ Explanation of the model  
   - Implementation code  
   - Example dataset (if applicable)  

---

